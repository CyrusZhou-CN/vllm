# SPDX-License-Identifier: Apache-2.0
import torch


def compiled_softmax(logits: torch.Tensor) -> torch.Tensor:
    """Faster softmax kernel generated by torch.compile.

    Args:
        logits: [n, vocab_size]
    """
    # NOTE(woosuk): Avoid recompilation by marking the first dim as dynamic.
    torch._dynamo.mark_dynamic(logits, index=0)
    return _softmax(logits)


@torch.compile
def _softmax(logits: torch.Tensor) -> torch.Tensor:
    return torch.softmax(logits, dim=-1, dtype=torch.float32)
